name: Build & Upload Offline Installer Archives

on:
  workflow_call:
    inputs:
      ref:
        required: true
        type: string
      run_id:
        required: true
        type: string
      idf_version:
        required: false
        type: string
        description: "Specific IDF version to build (e.g., v5.1.2). If empty, builds all versions."
    secrets:
      AWS_ACCESS_KEY_ID:
        required: true
      AWS_SECRET_ACCESS_KEY:
        required: true
      DL_DISTRIBUTION_ID:
        required: true

  workflow_dispatch:
    inputs:
      run_id:
        description: "The run id from which to take binaries"
        required: true
        type: string
      idf_version:
        description: "Specific IDF version to build (e.g., v5.1.2). Leave empty to build all."
        required: false
        type: string
      purge_all:
        description: "Purge all existing archives from S3 before upload (DANGEROUS)"
        required: false
        type: boolean
        default: false

  release:
    types: [published]

jobs:
  build-and-upload:
    name: Build & Upload (${{ matrix.package_name }})
    runs-on: ${{ matrix.os }}
    continue-on-error: false

    strategy:
      fail-fast: false
      matrix:
        include:
          - os: ubuntu-latest
            package_name: linux-x64
          - os: ubuntu-24.04-arm
            package_name: linux-aarch64
          - os: windows-latest
            package_name: windows-x64
          - os: macos-latest
            package_name: macos-aarch64
          - os: macos-13
            package_name: macos-x64

    steps:
      - name: Checkout (for scripts if needed)
        uses: actions/checkout@v4

      - name: Download offline_installer_builder artifact
        uses: actions/download-artifact@v5
        with:
          pattern: offline_installer_builder-${{ matrix.package_name }}-*
          merge-multiple: true
          path: ./
          github-token: ${{ secrets.GITHUB_TOKEN }}
          run-id: ${{ inputs.run_id || github.run_id }}

      - name: Make binary executable (Unix)
        if: runner.os != 'Windows'
        run: |
          chmod +x ./offline_installer_builder

      - name: Install UV (Python package manager)
        run: |
          cargo install --git https://github.com/astral-sh/uv uv

      - name: Set up AWS CLI
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-east-1

      - name: Download current offline_archives.json
        id: download_json
        run: |
          aws s3 cp s3://espdldata/dl/eim/offline_archives.json ./offline_archives.json 2>/dev/null || echo "[]" > ./offline_archives.json
          if ! jq -e 'type == "array"' ./offline_archives.json >/dev/null 2>&1; then
            echo "Invalid JSON, resetting to empty array"
            echo "[]" > ./offline_archives.json
          fi
          echo "Current offline_archives.json:"
          cat ./offline_archives.json

      - name: Purge existing archives (if requested)
        if: github.event_name == 'release' || inputs.purge_all == true
        run: |
          echo "Purging existing archives from S3..."
          jq -r '.[] | .filename' ./offline_archives.json | while read filename; do
            if [ -n "$filename" ]; then
              echo "Deleting s3://espdldata/dl/eim/$filename"
              aws s3 rm "s3://espdldata/dl/eim/$filename" || echo "Failed to delete $filename, continuing..."
            fi
          done

      - name: Build archives
        id: build
        run: |
          # Determine binary name
          if [ "${{ runner.os }}" = "Windows" ]; then
            BINARY="./offline_installer_builder.exe"
          else
            BINARY="./offline_installer_builder"
          fi

          # Determine arguments
          if [ -n "${{ inputs.idf_version }}" ]; then
            echo "Building specific version: ${{ inputs.idf_version }}"
            ARGS="-c default --idf-version-override ${{ inputs.idf_version }}"
          else
            echo "Building ALL versions"
            ARGS="-c default --build-all-versions"
          fi

          # Run it
          $BINARY $ARGS

          # Collect and rename all built .zst files to include platform
          PLATFORM="${{ matrix.package_name }}"
          mkdir -p built_archives

          for file in archive_v*.zst; do
            if [ ! -f "$file" ]; then continue; fi

            # Extract version: archive_v5.1.2.zst → 5.1.2
            VERSION=$(echo "$file" | sed -E 's/archive_v(.*)\.zst/\1/')
            NEW_NAME="archive_v${VERSION}_${PLATFORM}.zst"

            echo "Renaming $file → $NEW_NAME"
            mv "$file" "built_archives/$NEW_NAME"
          done

          # List built files
          echo "Built archives:"
          ls -la built_archives/

          # Fail if nothing built
          if [ ! "$(ls -A built_archives/ 2>/dev/null)" ]; then
            echo "ERROR: No archives built!" >&2
            exit 1
          fi

        shell: bash

      - name: Upload archives to S3 and generate build info
        id: upload
        run: |
          PLATFORM="${{ matrix.package_name }}"
          mkdir -p build-infos

          # Upload each .zst and record metadata
          for archive in built_archives/*.zst; do
            if [ ! -f "$archive" ]; then continue; fi

            # Extract version from new filename: archive_v5.1.2_linux-x64.zst → 5.1.2
            FILENAME=$(basename "$archive")
            VERSION=$(echo "$FILENAME" | sed -E 's/archive_v([^_]+)_.*/\1/')

            # Get file size
            if [ "${{ runner.os }}" = "macOS" ]; then
              SIZE=$(stat -f %z "$archive")
            else
              SIZE=$(stat -c %s "$archive")
            fi

            # Upload to S3
            aws s3 cp --acl=public-read "$archive" "s3://espdldata/dl/eim/$FILENAME"

            # Generate build info
            jq -n \
              --arg version "$VERSION" \
              --arg platform "$PLATFORM" \
              --arg filename "$FILENAME" \
              --argjson size $SIZE \
              '{"version": $version, "platform": $platform, "filename": $filename, "size": $size}' \
              > "build-infos/build-info-${VERSION}-${PLATFORM}.json"

            echo "Uploaded $FILENAME ($SIZE bytes)"
          done

      - name: Save build infos as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-infos-${{ matrix.package_name }}
          path: build-infos/
          retention-days: 7

  update-json:
    needs: build-and-upload
    runs-on: ubuntu-latest
    steps:
      - name: Set up AWS CLI
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-east-1

      - name: Download current offline_archives.json
        run: |
          aws s3 cp s3://espdldata/dl/eim/offline_archives.json ./offline_archives.json 2>/dev/null || echo "[]" > ./offline_archives.json
          if ! jq -e 'type == "array"' ./offline_archives.json >/dev/null 2>&1; then
            echo "[]" > ./offline_archives.json
          fi

      - name: Download all build infos
        uses: actions/download-artifact@v4
        with:
          pattern: build-infos-*
          path: ./all-build-infos/
          merge-multiple: true

      - name: Merge and update JSON
        run: |
          # Collect all new entries
          NEW_ENTRIES="[]"
          for info_file in all-build-infos/build-infos/*/build-info-*.json; do
            if [ -f "$info_file" ]; then
              echo "Processing: $info_file"
              entry=$(cat "$info_file")
              NEW_ENTRIES=$(echo "$NEW_ENTRIES" | jq --argjson entry "$entry" '. + [$entry]')
            fi
          done

          # Load current JSON
          CURRENT=$(cat offline_archives.json)

          # Remove existing entries that are being replaced (same platform + version)
          # Keep entries for platforms/versions NOT rebuilt
          UPDATED=$(jq --argjson new "$NEW_ENTRIES" '
            . as $current |
            ($new | map({version, platform})) as $to_replace |
            ($current | map(select(
              (.version + "-" + .platform) as $key |
              ($to_replace | map(.version + "-" + .platform) | index($key)) | not
            ))) + $new
          ' offline_archives.json)

          echo "Final offline_archives.json:"
          echo "$UPDATED" | jq .

          # Validate
          echo "$UPDATED" > updated_offline_archives.json
          if ! jq empty updated_offline_archives.json; then
            echo "Generated JSON is invalid!" >&2
            exit 1
          fi

          # Upload
          aws s3 cp --acl=public-read updated_offline_archives.json s3://espdldata/dl/eim/offline_archives.json

          # Invalidate CloudFront
          aws cloudfront create-invalidation \
            --distribution-id ${{ secrets.DL_DISTRIBUTION_ID }} \
            --paths "/dl/eim/offline_archives.json" "/dl/eim/archive_*.zst"

      - name: Upload final JSON as artifact
        uses: actions/upload-artifact@v4
        with:
          name: final-offline-archives-json
          path: updated_offline_archives.json

  autotest:
    needs: [build-and-upload, update-json]
    if: always() && needs.build-and-upload.result == 'success'
    uses: ./.github/workflows/test_offline.yml
    with:
      ref: ${{ inputs.ref || github.ref }}
      run_id: ${{ github.run_id }}
